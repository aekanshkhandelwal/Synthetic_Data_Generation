{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fraud Detection Synthetic Data Generation (Generator–Discriminator)\n",
                "\n",
                "This notebook implements a pipeline to generate synthetic fraud transaction data using a Conditional GAN (CTGAN)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment & Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries\n",
                "!pip install ctgan pandas numpy scikit-learn matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from ctgan import CTGAN\n",
                "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
                "\n",
                "# Load dataset\n",
                "file_path = 'crypto_scam_transaction_dataset.csv'\n",
                "df = pd.read_csv(file_path)\n",
                "\n",
                "print(\"Dataset Shape:\", df.shape)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle missing values\n",
                "print(\"Missing values before imputation:\\n\", df.isnull().sum())\n",
                "\n",
                "# Impute numerical columns with median\n",
                "numerical_cols_with_na = ['gas_fee_usd', 'avg_txn_interval_sender_min']\n",
                "for col in numerical_cols_with_na:\n",
                "    if col in df.columns:\n",
                "        df[col] = df[col].fillna(df[col].median())\n",
                "\n",
                "# Impute categorical columns with 'Unknown'\n",
                "categorical_cols_with_na = ['platform']\n",
                "for col in categorical_cols_with_na:\n",
                "    if col in df.columns:\n",
                "        df[col] = df[col].fillna('Unknown')\n",
                "\n",
                "print(\"Missing values after imputation:\\n\", df.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure is_scam is the target label\n",
                "target = 'is_scam'\n",
                "\n",
                "# Drop transaction_id as it's not a feature for generation\n",
                "if 'transaction_id' in df.columns:\n",
                "    df = df.drop(columns=['transaction_id'])\n",
                "\n",
                "# Convert timestamp to numeric features\n",
                "# timestamps are likely Unix timestamps. We can extract hour of day and day of week.\n",
                "if 'timestamp' in df.columns:\n",
                "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
                "    df['hour'] = df['datetime'].dt.hour\n",
                "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
                "    \n",
                "    # Calculate time gap between transactions if applicable (sorting by timestamp first)\n",
                "    df = df.sort_values(by='timestamp')\n",
                "    df['time_gap'] = df['timestamp'].diff().fillna(0)\n",
                "    \n",
                "    # Drop original timestamp and temporary datetime column\n",
                "    df = df.drop(columns=['timestamp', 'datetime'])\n",
                "\n",
                "print(\"Columns after timestamp conversion:\", df.columns)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify categorical and numerical columns\n",
                "categorical_features = ['blockchain', 'transaction_type', 'token_type', 'platform']\n",
                "numerical_features = [\n",
                "    'sender_wallet_age_days', 'receiver_wallet_age_days', 'transaction_amount_usd', 'gas_fee_usd',\n",
                "    'num_prev_transactions_sender', 'num_prev_transactions_receiver', 'avg_txn_interval_sender_min',\n",
                "    'failed_txn_ratio_sender', 'velocity_score', 'anomaly_score', 'time_gap'\n",
                "]\n",
                "\n",
                "# 'is_cross_chain' is binary, can be treated as categorical or numeric. Let's treat as categorical for CTGAN conditioning if needed, or just numeric binary.\n",
                "# Let's verify existing columns match our list\n",
                "existing_cols = df.columns.tolist()\n",
                "categorical_features = [col for col in categorical_features if col in existing_cols]\n",
                "numerical_features = [col for col in numerical_features if col in existing_cols]\n",
                "\n",
                "print(\"Categorical Features:\", categorical_features)\n",
                "print(\"Numerical Features:\", numerical_features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode categorical features (optional for CTGAN as it handles them, but good practice if using other models later)\n",
                "# CTGAN natively handles categorical columns if we pass them in 'discrete_columns'.\n",
                "# However, the request asks to \"Encode categorical features\". Let's use Label Encoding for simplicity if we were to pass to a standard non-GAN model,\n",
                "# but for CTGAN, it's BEST to leave them as strings or object types and specify them as discrete_columns.\n",
                "# WE WILL SKIP EXPLICIT ENCODING FOR CTGAN input to allow it to learn the categories, \n",
                "# BUT we will ensure they are object/string type.\n",
                "\n",
                "for col in categorical_features:\n",
                "    df[col] = df[col].astype(str)\n",
                "\n",
                "# Scale numerical features\n",
                "# CTGAN also handles normalization internally (using ModeSpecificNormalization).\n",
                "# Explicit scaling might interfere with CTGAN's internal processing if we want to reverse it easily for the synthetic data.\n",
                "# We will SKIP manual scaling to let CTGAN handle the distributions effectively, \n",
                "# or we can scale and then remember to inverse transform. \n",
                "# Given the prompt \"Scale numerical features\", we will apply MinMax scaling but keep the scaler to inverse transform later.\n",
                "\n",
                "scaler = MinMaxScaler()\n",
                "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
                "\n",
                "print(\"Data scaled. Example:\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train Generator–Discriminator (CTGAN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define discrete columns which are categorical + boolean/binary columns that are not continuous\n",
                "discrete_columns = categorical_features + ['is_cross_chain', 'is_scam', 'hour', 'day_of_week']\n",
                "# Ensure these are in the dataframe\n",
                "discrete_columns = [col for col in discrete_columns if col in df.columns]\n",
                "\n",
                "# Initialize CTGAN\n",
                "ctgan = CTGAN(epochs=10, verbose=True) # epochs set to 10 for quick demonstration, increase for better quality\n",
                "\n",
                "print(\"Training CTGAN on real data...\")\n",
                "ctgan.fit(df, discrete_columns=discrete_columns)\n",
                "print(\"Training Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generate Synthetic Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Condition generation on is_scam = 1\n",
                "num_samples = 1000 # Generate 1000 synthetic scam transactions\n",
                "\n",
                "# CTGAN sample method doesn't support direct conditional sampling in the basic API in older versions, \n",
                "# but newer versions might. \n",
                "# If CTGAN doesn't support `conditional_column` arg directly in `sample`, we might need to use `sample` and filter,\n",
                "# OR use the conditional vector logic if taking a deeper approach.\n",
                "# However, typically standard CTGAN generates the whole distribution. \n",
                "# To properly condition, we can use the 'sample' method which generates from the learned distribution.\n",
                "# If we need specific 'is_scam=1', we can standardly generate a batch and filter, or use the `conditional` parameters if available in the specific library version installed.\n",
                "# Let's assume standard sampling and we filter for now, or we can retrain on ONLY scam data if we strictly want a scam generator.\n",
                "# STRATEGY: Train on FULL data (as requested), and then we can try to sample.\n",
                "# NOTE: SDV (Synthetic Data Vault) wrapper around CTGAN has easier conditional sampling. CTGAN raw might need more manual work.\n",
                "# Let's try to generate a larger batch and filter, or check if we can enforce conditions.\n",
                "\n",
                "# Generating synthetic data\n",
                "synthetic_data = ctgan.sample(num_samples * 5) # Generate more to ensure we get enough scam labels if the model learned the ratio\n",
                "\n",
                "# Filter for scam transactions\n",
                "synthetic_scam_data = synthetic_data[synthetic_data['is_scam'] == 1]\n",
                "\n",
                "# If we didn't get enough, we might just take what we have or generate more.\n",
                "if len(synthetic_scam_data) < num_samples:\n",
                "    print(f\"Warning: Only generated {len(synthetic_scam_data)} scam samples. You may need to train longer or generate more samples.\")\n",
                "else:\n",
                "    synthetic_scam_data = synthetic_scam_data.sample(num_samples)\n",
                "\n",
                "print(f\"Generated {len(synthetic_scam_data)} synthetic scam transactions.\")\n",
                "synthetic_scam_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Combine & Validate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inverse transform the numerical columns for both real and synthetic data validation\n",
                "real_data_rescaled = df.copy()\n",
                "real_data_rescaled[numerical_features] = scaler.inverse_transform(real_data_rescaled[numerical_features])\n",
                "\n",
                "synthetic_scam_rescaled = synthetic_scam_data.copy()\n",
                "synthetic_scam_rescaled[numerical_features] = scaler.inverse_transform(synthetic_scam_rescaled[numerical_features])\n",
                "\n",
                "# Combine real non-scam + synthetic scam data\n",
                "real_non_scam = real_data_rescaled[real_data_rescaled['is_scam'] == 0]\n",
                "final_dataset = pd.concat([real_non_scam, synthetic_scam_rescaled], axis=0)\n",
                "\n",
                "# Shuffle dataset\n",
                "final_dataset = final_dataset.sample(frac=1).reset_index(drop=True)\n",
                "\n",
                "print(\"Final Dataset Shape:\", final_dataset.shape)\n",
                "print(\"Class Distribution:\\n\", final_dataset['is_scam'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate distributions (optional visualization)\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(data=real_data_rescaled, x='transaction_amount_usd', hue='is_scam', element='step', stat='density', common_norm=False)\n",
                "plt.title('Real Data: Transaction Amount Distribution')\n",
                "plt.show()\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(data=final_dataset, x='transaction_amount_usd', hue='is_scam', element='step', stat='density', common_norm=False)\n",
                "plt.title('Hybrid Data (Real Normal + Synthetic Fraud): Transaction Amount Distribution')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final fraud dataset\n",
                "final_dataset.to_csv('synthetic_fraud_dataset.csv', index=False)\n",
                "print(\"Saved final dataset to 'synthetic_fraud_dataset.csv'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}